{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from chat import *\n",
    "import openai\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "APIKEY = \"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\"\n",
    "openai.api_key = APIKEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"content\": \"Hello there! How may I assist you today?\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"Hello! How may I assist you today?\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"As an AI language model, I don't have feelings, but I'm functioning properly. Thank you for asking! How can I assist you today?\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"As an AI language model, I do not have feelings or emotions, but I am functioning well. Thank you for asking! How may I assist you today?\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"I'm an AI language model, so I don't have feelings or emotions, but I'm functioning well and ready to assist you with anything you need. How can I help you?\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"As an AI language model, I do not have feelings or emotions, but I am always ready and available to assist you with whatever you need. How may I help?\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"As an AI language model, I do not have feelings in the human sense, but I am functioning optimally. Thank you for asking. How may I assist you today?\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"As an AI language model, I don't have emotions like humans do. But I am here to assist you with anything you need help with. How can I assist you today?\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"As an AI language model, I do not have emotions like humans do. However, I am fully functioning and ready to assist with any questions or tasks you may have. How can I assist you today?\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n",
      "{\n",
      "  \"content\": \"I'm good, thank you for asking. As an AI language model, I don't have the capacity to feel emotions, but I'm always here to help you with any questions or tasks you might have.\",\n",
      "  \"role\": \"assistant\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import concurrent.futures\n",
    "\n",
    "openai.api_key = APIKEY\n",
    "\n",
    "# Set the list of prompts\n",
    "prompts = [\n",
    "    \"Hey what's up?\",\n",
    "    \"Hi there\",\n",
    "    \"Hello, world!\",\n",
    "    \"How are you?\",\n",
    "    \"How are you?\",\n",
    "    \"How are you?\",\n",
    "    \"How are you?\",\n",
    "    \"How are you?\",\n",
    "    \"How are you?\",\n",
    "    \"How are you?\",\n",
    "]\n",
    "\n",
    "# Function to make API call for a single prompt\n",
    "def make_api_call(prompt):\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "        stop=None,\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are ChatGPT, a large language model trained by OpenAI. You are taking the dutch driving exam and wil be presented with what you see around you. Answer as concisely as possible and only take the dutch traffic rules in to consideration.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    response = completion[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    return response\n",
    "\n",
    "# Create a ThreadPoolExecutor for concurrent API calls\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit the API calls as asynchronous tasks\n",
    "    api_calls = [executor.submit(make_api_call, prompt) for prompt in prompts]\n",
    "\n",
    "    # Retrieve the results as they become available\n",
    "    for future in concurrent.futures.as_completed(api_calls):\n",
    "        generated_text = future.result()\n",
    "        print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[39mprint\u001b[39m(result)\n\u001b[0;32m     41\u001b[0m \u001b[39m# Run the event loop\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m asyncio\u001b[39m.\u001b[39;49mrun(run_event_loop())\n",
      "File \u001b[1;32mc:\\Users\\Martijn\\miniconda3\\envs\\BEP\\lib\\asyncio\\runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[39mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m coroutines\u001b[39m.\u001b[39miscoroutine(main):\n\u001b[0;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39ma coroutine was expected, got \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(main))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set the list of prompts\n",
    "prompts = [\n",
    "    \"Once upon a time\",\n",
    "    \"In a galaxy far, far away\",\n",
    "    \"Hello, world!\"\n",
    "]\n",
    "\n",
    "# Function to make API call asynchronously\n",
    "async def make_api_call(prompt):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        url = 'https://api.openai.com/v1/engines/davinci-codex/completions'\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Authorization': 'Bearer ' + openai.api_key\n",
    "        }\n",
    "        params = {\n",
    "            'prompt': prompt,\n",
    "            'max_tokens': 50\n",
    "        }\n",
    "\n",
    "        async with session.post(url, headers=headers, json=params) as response:\n",
    "            data = await response.json()\n",
    "            generated_text = data['choices'][0]['message']['content']\n",
    "            return generated_text\n",
    "\n",
    "# Create a coroutine to run the event loop\n",
    "async def run_event_loop():\n",
    "    tasks = [make_api_call(prompt) for prompt in prompts]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # Print the results\n",
    "    for result in results:\n",
    "        print(result)\n",
    "\n",
    "# Run the event loop\n",
    "asyncio.run(run_event_loop())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.ChatCompletion.create(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"vraag 1.jpg\": \"if you understand, return 1\",\n",
    "          \"vraag 2.jpg\": \"if you understand, return 2\",\n",
    "          \"vraag 3.jpg\": \"if you understand, return 3\",\n",
    "          \"vraag 4.jpg\": \"if you understand, return 4\",\n",
    "          \"vraag 5.jpg\": \"if you understand, return 5\",\n",
    "          \"vraag 6.jpg\": \"if you understand, return 6\",\n",
    "          \"vraag 7.jpg\": \"if you understand, return 7\",\n",
    "          \"vraag 8.jpg\": \"if you understand, return 8\",\n",
    "          \"vraag 9.jpg\": \"if you understand, return 9\",\n",
    "          \"vraag 10.jpg\": \"if you understand, return 10\",}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_with_backoff(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        max_tokens=1024,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are ChatGPT, a large language model trained by OpenAI. You are taking the dutch driving exam and wil be presented with what you see around you. Answer as concisely as possible and only take the dutch traffic rules in to consideration.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error ocurred: RetryError[<Future at 0x1b41d5247c0 state=finished raised TypeError>]\n",
      "Exception occured: local variable 'response' referenced before assignment\n",
      "An error ocurred: RetryError[<Future at 0x1b41d4649d0 state=finished raised TypeError>]\n",
      "Exception occured: local variable 'response' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import os\n",
    "from aiohttp import ClientSession\n",
    "\n",
    "async def get_chat_response(request):\n",
    "    try:\n",
    "        response = await completion_with_backoff(request)\n",
    "    except Exception as err:\n",
    "        print(f\"An error ocurred: {err}\")\n",
    "    return response\n",
    "\n",
    "\n",
    "async def run_program(request):\n",
    "    \"\"\"Wrapper for running program in an asynchronous manner\"\"\"\n",
    "    try:\n",
    "        response = await get_chat_response(request)\n",
    "        print(response)\n",
    "    except Exception as err:\n",
    "        print(f\"Exception occured: {err}\")\n",
    "        pass\n",
    "\n",
    "async with ClientSession() as session:\n",
    "    await asyncio.gather(*[run_program(prompt) for prompt in inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd # Make a dataframe\n",
    "import aiohttp  # for making API calls concurrently\n",
    "import asyncio  # for running API calls concurrently\n",
    "\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "request_url =  \"https://api.openai.com/v1/completions\"\n",
    "request_header = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "#data\n",
    "prompt_list = ['Door', 'Window', 'Table']\n",
    "init_context = \" height if the room contains a door which is 8ft in height, a table 2ft in height and a window 6ft in height\"\n",
    "\n",
    "#dataframe of list of questions\n",
    "q_dataframe = pd.DataFrame({\"type\": prompt_list})\n",
    "\n",
    "async def process_question(question: str, context: str):\n",
    "    query = \"What is the \" + question + context\n",
    "    data = {'model': 'text-davinci-003', 'prompt': f'{query}'}\n",
    "    \n",
    "    try:\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(url=request_url, headers=request_header, json=data) as response:\n",
    "                resp = await response.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed {e}\")\n",
    "    \n",
    "async def process_questions(idf):\n",
    "    results = await asyncio.gather(*[process_question(question, init_context) for question in idf['type']])\n",
    "    \n",
    "asyncio.create_task(process_questions(q_dataframe))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BEP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
